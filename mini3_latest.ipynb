{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, ActivityRegularization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD, Adadelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3007197650656760835\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5159190528\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2746153569715854015\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "GPU device name is: /device:GPU:0\n",
      "1.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"GPU device name is: \" + tf.test.gpu_device_name())\n",
    "print(tf.__version__)\n",
    "K.tensorflow_backend._get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur(i,j,train_images1,detected):        \n",
    "        detected.append([i,j])\n",
    "        if i + 1 < 63 and [i+1,j] not in detected:\n",
    "            if train_images1[i+1][j] == 1:\n",
    "                recur(i+1,j,train_images1,detected)\n",
    "        if j + 1 < 63 and [i,j+1] not in detected:\n",
    "            if train_images1[i][j+1] == 1:\n",
    "                recur(i,j+1,train_images1,detected)\n",
    "        if i - 1 > 0 and [i-1,j] not in detected:\n",
    "            if train_images1[i - 1][j] == 1:\n",
    "                recur(i-1,j,train_images1,detected)        \n",
    "        if j - 1 > 0 and [i,j-1] not in detected:\n",
    "            if train_images1[i][j - 1] == 1:\n",
    "                recur(i,j - 1,train_images1,detected)\n",
    "    \n",
    "\n",
    "def preprocess(input_images):       \n",
    "\n",
    "    croped = []\n",
    "    croped_image = []\n",
    "    for img_index in range(len(input_images)):\n",
    "        print(\"processing image \" + str(img_index)+ \" of \" + str(len(input_images)), end= '\\r')\n",
    "        input_images1= input_images[img_index] > 250\n",
    "    \n",
    "        \n",
    "        obj = [] \n",
    "        obj_number = -1              \n",
    "        for i in range(64):\n",
    "            for j in range(64):\n",
    "                detected = []\n",
    "                if obj_number == -1:\n",
    "                    if input_images1[i][j] == 1:\n",
    "                        recur(i,j,input_images1,detected)\n",
    "                        down = max([_[0] for _ in detected])\n",
    "                        up = min([_[0] for _ in detected])\n",
    "                        right = max([_[1] for _ in detected])\n",
    "                        left = min([_[1] for _ in detected])\n",
    "\n",
    "                        if down-up+1 > right-left+1:\n",
    "                            area =  (down-up+1) **2\n",
    "                        elif down-up+1 <= right-left+1:\n",
    "                             area =  (right-left+1) **2\n",
    "                        obj.append([up,down,left,right,area])\n",
    "                        obj_number += 1\n",
    "                \n",
    "                elif input_images1[i][j] == 1 and not (i in range(obj[obj_number][0],obj[obj_number][1]+1) and j in range(obj[obj_number][2],obj[obj_number][3]+1)) :\n",
    "                    recur(i,j,input_images1,detected)\n",
    "                    down = max([_[0] for _ in detected])\n",
    "                    up = min([_[0] for _ in detected])\n",
    "                    right = max([_[1] for _ in detected])\n",
    "                    left = min([_[1] for _ in detected])\n",
    "                  \n",
    "                    if down-up+1 > right-left+1:\n",
    "                            area =  (down-up+1) **2\n",
    "                    elif down-up+1 <= right-left+1:\n",
    "                            area =  (right-left+1) **2   \n",
    "                    obj.append([up,down,left,right,area])\n",
    "                    obj_number += 1\n",
    "                \n",
    "                \n",
    "        maximum = -1 \n",
    "        for i in range(len(obj)):\n",
    "                    \n",
    "            if obj[i][4] > maximum:\n",
    "                maximum = obj[i][4]\n",
    "                index = i        \n",
    "    \n",
    "        croped.append(obj[index])\n",
    "        \n",
    "        croped_image.append(input_images1[obj[index][0]:obj[index][1]+1,obj[index][2]:obj[index][3]+1])\n",
    "        \n",
    "        while np.shape(croped_image[img_index])[0] < 64:\n",
    "           croped_image[img_index] = np.insert(croped_image[img_index],0,np.zeros(np.shape(croped_image[img_index])[1]),axis = 0)\n",
    "           if np.shape(croped_image[img_index])[0] < 64:\n",
    "                croped_image[img_index] = np.insert(croped_image[img_index],np.shape(croped_image[img_index])[0],np.zeros(np.shape(croped_image[img_index])[1]),axis = 0)\n",
    "        \n",
    "        while np.shape(croped_image[img_index])[1] < 64:\n",
    "            croped_image[img_index] = np.insert(croped_image[img_index],0,np.zeros(np.shape(croped_image[img_index])[0]),axis = 1)\n",
    "            if np.shape(croped_image[img_index])[1] < 64:\n",
    "               croped_image[img_index] = np.insert(croped_image[img_index],np.shape(croped_image[img_index])[1],np.zeros(np.shape(croped_image[img_index])[0]),axis = 1)\n",
    "    \n",
    "    croped_image = np.asarray(croped_image)\n",
    "    \n",
    "    return croped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Training shape: (37500, 64, 64, 1)\n",
      "X_Validation shape: (2500, 64, 64, 1)\n",
      "Y_Training shape: (37500, 10)\n",
      "Y_Validation shape: (2500, 10)\n",
      "X_Test shape: (10000, 64, 64, 1)\n",
      "Number of images in training set 37500\n",
      "Number of images in validation set 2500\n",
      "Number of images in test set 10000\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and splitting \n",
    "\n",
    "num_classes = 10\n",
    "x_train = pd.read_pickle('./datasets/train_processed1.pkl')\n",
    "# x_train = np.load('./datasets/train_processed1.npy')\n",
    "# x_test = pd.read_pickle('./datasets/test_images.pkl')\n",
    "Y_train = pd.read_csv('./datasets/train_labels.csv')\n",
    "x_test = np.load('./datasets/test_processed1.npy')\n",
    "\n",
    "# Preprocessing \n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255  \n",
    "x_train = np.asarray(x_train)\n",
    "# x_train = preprocess(x_train)\n",
    "\n",
    "\n",
    "# x_test = pd.read_pickle('./datasets/test_images.pkl')\n",
    "# x_test = preprocess(x_test)\n",
    "\n",
    "# chage csv format to numpy array for training labels\n",
    "\n",
    "y_train = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    y_train.append(Y_train.iloc[i]['Category'])\n",
    "    \n",
    "y_train = np.asarray(y_train)\n",
    "y_train = y_train.reshape(np.shape(y_train)[0],1)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "\n",
    "img_rows = x_train.shape[1]\n",
    "img_cols = x_train.shape[2]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "X_Test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Splitting training data into Train-Dev \n",
    "\n",
    "X_Training, X_Validation, Y_Training, Y_Validation = train_test_split(x_train, y_train, test_size = 0.0625)\n",
    "\n",
    "print('X_Training shape:', X_Training.shape)\n",
    "print('X_Validation shape:', X_Validation.shape)\n",
    "print('Y_Training shape:', Y_Training.shape)\n",
    "print('Y_Validation shape:', Y_Validation.shape)\n",
    "print('X_Test shape:', X_Test.shape)\n",
    "print('Number of images in training set', X_Training.shape[0])\n",
    "print('Number of images in validation set', X_Validation.shape[0])\n",
    "print('Number of images in test set', X_Test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_mnist shape: (60000, 64, 64, 1)\n",
      "60000 mnist train samples\n",
      "10000 mnist test samples\n"
     ]
    }
   ],
   "source": [
    "# Original mnist data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "img_rows_mnist, img_cols_mnist = x_train_mnist.shape[1], x_train_mnist.shape[1]\n",
    "\n",
    "    \n",
    "x_train_mnist = x_train_mnist.astype('float32')\n",
    "x_test_mnist = x_test_mnist.astype('float32')\n",
    "x_train_mnist /= 255\n",
    "x_test_mnist /= 255    \n",
    "\n",
    "y_train_mnist = keras.utils.to_categorical(y_train_mnist, num_classes)\n",
    "y_test_mnist = keras.utils.to_categorical(y_train_mnist, num_classes)\n",
    "\n",
    "train = np.zeros((x_train_mnist.shape[0],img_rows,img_cols))\n",
    "test = np.zeros((x_test_mnist.shape[0],img_rows,img_cols))\n",
    "\n",
    "for i in range(x_train_mnist.shape[0]):\n",
    "    train[i,:,:] = np.pad(x_train_mnist[i,:,:],pad_width= 18, mode= 'constant', constant_values=0)\n",
    "    \n",
    "for i in range(x_test_mnist.shape[0]):\n",
    "    test[i,:,:] = np.pad(x_test_mnist[i,:,:],pad_width= 18, mode= 'constant', constant_values=0)\n",
    "\n",
    "x_train_mnist = train.reshape(x_train_mnist.shape[0], img_rows, img_cols,1)\n",
    "x_test_mnist = test.reshape(x_test_mnist.shape[0], img_rows, img_cols,1)\n",
    "    \n",
    "print('x_train_mnist shape:', x_train_mnist.shape)\n",
    "print(x_train_mnist.shape[0], 'mnist train samples')\n",
    "print(x_test_mnist.shape[0], 'mnist test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the required Keras modules containing model and layers\n",
    "def model1():\n",
    "\n",
    "    # Creating a Sequential Model and adding the layers\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(5,5)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(5,5)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(3,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(4,4)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "#     model.add(Conv2D(1024, kernel_size=(3,3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "    model.add(Dense(1024, activation=tf.nn.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "#     model.add(Dense(128, activation=tf.nn.relu))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10,activation=tf.nn.softmax)) # Since we have ten classes\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the required Keras modules containing model and layers\n",
    "def model2():\n",
    "\n",
    "    # Creating a Sequential Model and adding the layers\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(7,7)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(5,5)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.05))\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(3,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "\n",
    "    \n",
    "#     model.add(Conv2D(128, kernel_size=(4,4)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "#     model.add(Conv2D(1024, kernel_size=(3,3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "    model.add(Dense(1024, activation=tf.nn.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "#     model.add(Dense(128, activation=tf.nn.relu))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(10,activation=tf.nn.softmax)) # Since we have ten classes\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the required Keras modules containing model and layers\n",
    "def model3():\n",
    "\n",
    "    # Creating a Sequential Model and adding the layers\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3,3),input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3,3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(128, kernel_size=(3,3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "   \n",
    "#     model.add(Conv2D(128, kernel_size=(3,3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "#     model.add(ActivityRegularization(l1=0.01, l2=0.01))\n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "#     model.add(Dense(1024, activation=tf.nn.relu))\n",
    "#     model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, activation=tf.nn.relu))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(10,activation=tf.nn.softmax)) # Since we have ten classes\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67500\n"
     ]
    }
   ],
   "source": [
    "augment_size = 30000\n",
    "image_generator = ImageDataGenerator(\n",
    "            rotation_range=10,\n",
    "            zoom_range = 0.05, \n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False)\n",
    "# fit data for zca whitening\n",
    "image_generator.fit(X_Training, augment=True)\n",
    "# get transformed images\n",
    "randidx = np.random.randint(X_Training.shape[0], size=augment_size)\n",
    "x_augmented = X_Training[randidx].copy()\n",
    "y_augmented = Y_Training[randidx].copy()\n",
    "x_augmented = image_generator.flow(x_augmented, np.zeros(augment_size),\n",
    "                            batch_size=augment_size, shuffle=False).next()[0]\n",
    "# append augmented data to trainset\n",
    "X_Tr = np.concatenate((X_Training, x_augmented))\n",
    "Y_Tr = np.concatenate((Y_Training, y_augmented))\n",
    "print(X_Tr.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37500/37500 [==============================] - 13s 347us/step - loss: 0.6760 - acc: 0.7931\n",
      "Epoch 2/20\n",
      "37500/37500 [==============================] - 12s 325us/step - loss: 0.3727 - acc: 0.8985\n",
      "Epoch 3/20\n",
      "37500/37500 [==============================] - 12s 328us/step - loss: 0.3193 - acc: 0.9155\n",
      "Epoch 4/20\n",
      "37500/37500 [==============================] - 12s 330us/step - loss: 0.2885 - acc: 0.9226\n",
      "Epoch 5/20\n",
      "37500/37500 [==============================] - 13s 336us/step - loss: 0.2647 - acc: 0.9289\n",
      "Epoch 6/20\n",
      "37500/37500 [==============================] - 13s 334us/step - loss: 0.2432 - acc: 0.9348\n",
      "Epoch 7/20\n",
      "37500/37500 [==============================] - 13s 345us/step - loss: 0.2294 - acc: 0.9398\n",
      "Epoch 8/20\n",
      "37500/37500 [==============================] - 12s 332us/step - loss: 0.2146 - acc: 0.9411\n",
      "Epoch 9/20\n",
      "37500/37500 [==============================] - 12s 332us/step - loss: 0.2008 - acc: 0.9453\n",
      "Epoch 10/20\n",
      "37500/37500 [==============================] - 12s 325us/step - loss: 0.1901 - acc: 0.9482\n",
      "Epoch 11/20\n",
      "37500/37500 [==============================] - 12s 330us/step - loss: 0.1834 - acc: 0.9500\n",
      "Epoch 12/20\n",
      "37500/37500 [==============================] - 12s 330us/step - loss: 0.1733 - acc: 0.9525\n",
      "Epoch 13/20\n",
      "37500/37500 [==============================] - 12s 329us/step - loss: 0.1586 - acc: 0.9558\n",
      "Epoch 14/20\n",
      "37500/37500 [==============================] - 12s 329us/step - loss: 0.1569 - acc: 0.9546\n",
      "Epoch 15/20\n",
      "37500/37500 [==============================] - 12s 331us/step - loss: 0.1468 - acc: 0.9581\n",
      "Epoch 16/20\n",
      "37500/37500 [==============================] - 13s 337us/step - loss: 0.1354 - acc: 0.9609\n",
      "Epoch 17/20\n",
      "37500/37500 [==============================] - 13s 339us/step - loss: 0.1339 - acc: 0.9609\n",
      "Epoch 18/20\n",
      "37500/37500 [==============================] - 13s 336us/step - loss: 0.1241 - acc: 0.9628\n",
      "Epoch 19/20\n",
      "37500/37500 [==============================] - 13s 338us/step - loss: 0.1245 - acc: 0.9634\n",
      "Epoch 20/20\n",
      "37500/37500 [==============================] - 13s 336us/step - loss: 0.1161 - acc: 0.9653\n",
      "\n",
      "Validation Accuracy for CNN is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.952"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model3()\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(optimizer='Adadelta', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x=X_Training, y=Y_Training, epochs = epochs, batch_size= batch_size)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "Y_Prediction = model.predict_classes(x= X_Validation, batch_size= batch_size)\n",
    "Y_Prediction = keras.utils.to_categorical(Y_Prediction, num_classes)\n",
    "print('')\n",
    "print('Validation Accuracy for CNN is: ') \n",
    "accuracy_score(Y_Validation, Y_Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 62, 62, 128)       1280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 29, 29, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1024)              2360320   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,482,570\n",
      "Trainable params: 2,482,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model4()\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(optimizer='adadelta', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "iterations= 3\n",
    "validation = np.zeros((1,Y_Training.shape[1]))\n",
    "\n",
    "# for i in range(0,iterations):\n",
    "\n",
    "model.fit(x=X_Training, y=Y_Training, epochs = epochs, batch_size= batch_size,shuffle=True)\n",
    "\n",
    "middle = model.predict(x= X_Validation, batch_size= batch_size)\n",
    "# validation = validation + middle\n",
    "\n",
    "model1 = \n",
    "    \n",
    "# Y_Prediction = keras.utils.to_categorical(validation, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validation set\n",
    "Y_Prediction = model.predict_classes(x= X_Validation, batch_size= batch_size)\n",
    "Y_Prediction = keras.utils.to_categorical(Y_Prediction, num_classes)\n",
    "print('')\n",
    "print('Validation Accuracy for CNN is: ') \n",
    "accuracy_score(Y_Validation, Y_Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfer Learning from original mnist model\n",
    "mymodel = model\n",
    "for layer in mymodel.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in mymodel.layers:\n",
    "    print(layer, layer.trainable)# Adding custom layers\n",
    "    \n",
    "final_model = Sequential()\n",
    "final_model.add(mymodel)\n",
    "# final_model.add(Flatten())\n",
    "# final_model.add(Dense(128, activation='relu'))\n",
    "# final_model.add(Dropout(0.5))\n",
    "# final_model.add(Dense(num_classes, activation='softmax'))\n",
    "final_model.summary()\n",
    "\n",
    "# final_model.compile(optimizer='adam', \n",
    "#               loss='categorical_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "# final_model.fit(x=x_train, y=y_train, epochs = epochs, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "\n",
    "final_model.compile(optimizer='adadelta', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "final_model.fit(x=X_Training, y=Y_Training, epochs = epochs, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)\n",
    "model_Deep = model2()\n",
    "epochs_Deep = 5\n",
    "batch_size = 128\n",
    "\n",
    "model_Deep.compile(optimizer='adadelta', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model_Deep.fit(x=x_train, y=y_train, epochs = epochs_Deep, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write test predictions to .csv file\n",
    "\n",
    "def csvWriter(prediction, submission_no):\n",
    "    index = 0\n",
    "    filename = 'G_40_submission_' + str(submission_no) + '.csv'\n",
    "    csv = open(filename, \"w\") \n",
    "\n",
    "    columnTitleRow = \"Id,Category\\n\"\n",
    "    csv.write(columnTitleRow)\n",
    "    \n",
    "    for i in prediction:\n",
    "        csv.write(str(index) + ',' + str(i) + \"\\n\")\n",
    "        index+=1\n",
    "    \n",
    "    csv.close()\n",
    "    \n",
    "# Test set prediction and creating csv for Kaggle submission\n",
    "\n",
    "Y_Test = model.predict_classes(X_Test, batch_size= batch_size)\n",
    "csvWriter(Y_Test,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inividual classification\n",
    "\n",
    "\n",
    "image_index = 830\n",
    "plt.imshow(x_test[image_index].reshape(img_rows, img_cols),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, img_rows, img_cols, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = X_Training.reshape(X_Training.shape[0],X_Training.shape[1],X_Training.shape[2])\n",
    "td = X_Validation.reshape(X_Validation.shape[0],X_Validation.shape[1],X_Validation.shape[2])\n",
    "xtr = np.zeros((X_Training.shape[0],X_Training.shape[1],X_Training.shape[2],3))\n",
    "xv = np.zeros((X_Validation.shape[0],X_Validation.shape[1],X_Validation.shape[2],3))\n",
    "\n",
    "for i in range(X_Training.shape[0]):\n",
    "    for j in range(3):\n",
    "        xtr[i,:,:,j] = md[i,...]\n",
    "        \n",
    "for i in range(X_Validation.shape[0]):\n",
    "    for j in range(3):\n",
    "        xv[i,:,:,j] = td[i,...]       \n",
    "        \n",
    "        \n",
    "print(xtr.shape)\n",
    "print(xv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "input_sh = X_Training[0,...]\n",
    "\n",
    "vgg_conv = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=input_sh)\n",
    "\n",
    "model1 = models.Sequential()\n",
    "model1.add(vgg_conv)\n",
    "model1.add(layers.Dense(256, activation='relu'))\n",
    "model1.add(layers.Dropout(0.5))\n",
    "model1.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model1.compile(optimizer=optimizers.Adam(lr=2e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    " \n",
    "history = model.fit(xtr,\n",
    "                    Y_Training,\n",
    "                    epochs=20,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(xv,Y_Validation))\n",
    "\n",
    "Y_Prediction = model.predict_classes(x= xv, batch_size= batch_size)\n",
    "Y_Prediction = keras.utils.to_categorical(Y_Prediction, num_classes)\n",
    "print('')\n",
    "print('Validation Accuracy for CNN is: ') \n",
    "accuracy_score(Y_Validation, Y_Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Training[30,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN-GPU(py35)",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
